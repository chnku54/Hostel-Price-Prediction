# -*- coding: utf-8 -*-
"""[Latest] Feature Engineering + Feature Selection + Regression + Params Optimization v5.ipynb  的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1UhPWibHCwdcGxmYH95c6f_ftmMafvrfg
"""

!pwd

from google.colab import drive

drive.mount('/content/drive')

!ls "/content/drive/My Drive/Colab Notebooks/MSBD5001"

!cp -r '/content/drive/My Drive/Colab Notebooks/MSBD5001/data' "./data"

!ls

import os
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
!pip install -q transformers
!pip install -q tqdm
!pip install -q py-translate
import pandas as pd
import numpy as np
from transformers import pipeline
from tqdm import tqdm
import torch
import gc
from transformers import AutoTokenizer, AutoModelForSequenceClassification

pd.set_option('display.max_rows', 500)

def describe_df(df):
  _temp = []
  for column in df.columns:
    _temp.append({
        "column": column,
        "type": df[column].dtype,
        "nan %": df[column].isna().mean(),
        "# unqiue": df[column].nunique(),
    })
  display(pd.DataFrame(_temp))

listing_df = pd.read_csv('./data/listings.csv.gz', compression='gzip')
listing_summary_df = pd.read_csv('./data/listings.csv')
calendar_df = pd.read_csv('./data/calendar.csv.gz', compression='gzip')
neighbourhoods_df = pd.read_csv('./data/neighbourhoods.csv')
neighbourhoods_geo_df = pd.read_json('./data/neighbourhoods.geojson')
reviews_df = pd.read_csv('./data/reviews.csv.gz', compression='gzip')
reviews_summary_df = pd.read_csv('./data/reviews.csv')

# translated chinese -> english:
listing_translated_df = pd.read_pickle('./data/trans_listing.pkl')

# image stats
image_stat_df = pd.read_csv('./data/image_stat.csv')

listing_df = pd.merge(listing_df, listing_translated_df, on="id", how="left")

listing_df = pd.merge(listing_df, image_stat_df, on="id", how="left")

# Commented out IPython magic to ensure Python compatibility.
# %timeit -n 1 -r 1 print(pipeline('sentiment-analysis', device=0)('we love you'))

# are all listing existing in both listing and listing summary df?
listing_df["id"].isin(listing_summary_df["id"]).mean() # yes

"""# Feature Engineering

## Feature Engineering TODO items:

- [Done] Categorical features (Label encoding)
- [Todo] Categorical features (One Hot) for Neural network
- [Done] Numerical features preprocessing
- [Done] Numerical features aggegraton (groupby) 
- [Done] Text columns' word count
- [Done] Text columns' TFIDF encoding
- [Done] Pretrain neural network Text Features
- [Todo] Image columns' meta features
- [Todo] Pretrain neural network Image Features
"""

import json
from pprint import pprint
from sklearn import preprocessing
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.decomposition import TruncatedSVD

def preprocess_features(listing_df, listing_summary_df):
  # merging listing and listing_summary df
  _df = listing_df.drop(columns=["price"])
  merging_columns = [col for col in listing_summary_df.columns if col not in _df.columns]
  _df = pd.merge(_df, listing_summary_df[merging_columns + ["id"]], how="left", on="id")

  # Remove columns with too many nans
  drop_cols = []
  for col in _df.columns:
    if _df[col].isna().mean() > .9:
      drop_cols.append(col)
  print("Drop", drop_cols)
  _df = _df.drop(columns=drop_cols)

  # Convert rates to float
  for rate_column in ["host_response_rate", "host_acceptance_rate"]:
    _df[rate_column] = _df[rate_column].str.replace("%", "").astype(float)

  # Convert money to float
  for price_column in [ "security_deposit", "cleaning_fee", "extra_people"]:
    _df[price_column] = _df[price_column].str.replace("$", "").str.replace(",", "").astype(float)
    _df[f"has_{price_column}"] = ~ _df[price_column].isna()
  
  # host_verifications
  # convert json to list column
  _df["host_verifications"] = _df["host_verifications"].apply(lambda x: json.loads(x.replace("\'", "\"")) if x != "None" else [])
  # find all possible verification methods
  verification_methods = set()
  for verification in _df["host_verifications"].values:
    for method in verification:
      verification_methods.add(method)      
  verification_methods = list(sorted(verification_methods))
  print(verification_methods)

  for method in verification_methods:
    _df[f"verified_by_{method}"] = _df["host_verifications"].apply(lambda lst: method in lst)

  _df = _df.drop(columns=["host_verifications"])

  # amenities
  _df["amenities"] = _df["amenities"].str.replace("{", "").str.replace("}", "").str.replace("\"", "")

  # drop outliers
  print(_df["price"].quantile(0.01), _df["price"].quantile(0.99))
  _df = _df[(_df["price"]>=_df["price"].quantile(0.01)) & (_df["price"]<=_df["price"].quantile(0.99))].reset_index(drop=True)

  return _df

def classify_columns(df):
  id_feature = "id"
  target_feature = "price"
  numerical_features = [
                        col for col in df.columns 
                        if "id" not in col and
                        col != target_feature and col != id_feature and
                        (df[col].dtype == "int" or df[col].dtype == "float")
                        
                      ]
  categorical_features = [
                        col for col in df.columns 
                        if "id" not in col and
                        col != target_feature and col != id_feature and
                        df[col].dtype == "object" and
                        df[col].nunique() <= 1000 and
                        df[col].nunique() > 2 
                      ]
  boolean_features = [
                        col for col in df.columns 
                        if "id" not in col and
                        col != target_feature and col != id_feature and
                        df[col].nunique() == 2 
                      ] 

  text_features = [
                        col for col in df.columns 
                        if "id" not in col and
                        col != target_feature and col != id_feature and
                        df[col].dtype == "object" and
                        df[col].nunique() > 1000  and "url" not in col and "trans_" not in col
                      ]
  img_features = [
                        col for col in df.columns 
                        if "id" not in col and
                        col != target_feature and col != id_feature and
                        df[col].dtype == "object" and
                        df[col].nunique() > 1000  and "url" in col
                      ]
  return {
      "boolean_features": boolean_features,
      "categorical_features": categorical_features,
      "numerical_features": numerical_features,
      "text_features": text_features,
      "img_features": img_features,
  }

def label_encode_categorical_features(df, columns):
  for col in columns:
    # fillna with unknown
    df[col] = df[col].fillna("unknown")
    le = preprocessing.LabelEncoder()
    df[col] = le.fit_transform(df[col])


def encode_boolean_features(df, columns):
  for col in columns:
    if df[col].dtype == 'object':
      df[col] = df[col].apply(lambda x: True if x == 't' else False)
  # convert boolean features to 0/1 int type
  df[columns] = df[columns].astype(int)

def preprocess_numerical_features(df, columns):
  # fillna
  df[columns] = df[columns].fillna(0)

def preprocess_text_features(df, columns):
  df[columns] = df[columns].fillna("")
  # for col in columns:
  #   df[col] = df[col].str.lower()


def word_counts(df, columns):
  for col in columns:
    df[f"word_count_{col}"] = df[col].str.split(" ").str.len()
    df[f"cap_count_{col}"] = df[col].apply(lambda x: sum(1 for c in x if c.isupper()))

def encode_tfidf_truncated_svd(df, columns, tfidf_size=200, svd_size=20, svd_n_iter=50):
  for col in columns:
    tfidf = TfidfVectorizer(
          min_df=5, max_features=tfidf_size, ngram_range=(1, 2),
          strip_accents='unicode',
          lowercase=True, analyzer='word', token_pattern=r'\w+',
          use_idf=True, smooth_idf=True, sublinear_tf=True, 
          stop_words = 'english'
        )
    svd = TruncatedSVD(n_components=svd_size, n_iter=svd_n_iter)
    
    tf_idf_encoding = tfidf.fit_transform(df[col])
    compressed_encoding = svd.fit_transform(tf_idf_encoding)
    print("tfidf", col, compressed_encoding.shape)
    for idx in range(compressed_encoding.shape[1]):
      df[f"tfidf_{idx}_{col}"] = compressed_encoding[:, idx]


def get_sentiments(df, columns):
  torch.cuda.empty_cache()
  gc.collect()
  # https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english
  tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")
  model = AutoModelForSequenceClassification.from_pretrained("distilbert-base-uncased-finetuned-sst-2-english")

  batch_size = 200
  for col in columns:
    print("Getting sentiments of", f"trans_{col}")
    values = []
    df[f"trans_{col}"] = df[f"trans_{col}"].fillna("")
    for i in tqdm(range(0, df.shape[0], batch_size)):
      start_idx = i
      end_idx = i + batch_size
      if i + batch_size >= df.shape[0]:
        end_idx = df.shape[0]
      
      inputs = list(df[f"trans_{col}"].str[0: 300].values[start_idx: end_idx])
      results = pipeline('sentiment-analysis', device=0,
                        model=model,
                        tokenizer=tokenizer)(
        inputs
      )
      values += [sa["score"] if sa["label"] == "POSITIVE" else -1 * sa["score"] for sa in results]
    df[f"sentiment_{col}"] = values


def mean_price_per_category(df, columns):
  for col in columns:
    df[f"mean_price_{col}"] = df.groupby(col)["price"].transform("mean")

def scale_features(df, columns):
  # standard scaling
  sc = preprocessing.StandardScaler()
  df[columns] = sc.fit_transform(df[columns])

train_df = preprocess_features(listing_df, listing_summary_df)
column_types = classify_columns(train_df)
column_types["text_features"] = [col for col in column_types["text_features"] if col not in ["host_name", "host_since", "first_review", "last_review"]]
pprint(column_types)



encode_boolean_features(train_df, column_types["boolean_features"])
label_encode_categorical_features(train_df, column_types["categorical_features"])
mean_price_per_category(train_df, column_types["categorical_features"])

preprocess_text_features(train_df, column_types["text_features"])
word_counts(train_df, column_types["text_features"])
preprocess_numerical_features(train_df, column_types["numerical_features"])

encode_tfidf_truncated_svd(train_df, column_types["text_features"], tfidf_size=200, svd_size=5, svd_n_iter=100)

get_sentiments(train_df, column_types["text_features"])

column_types["numerical_features"] += [col for col in train_df.columns if "word_count_" in col]
column_types["numerical_features"] += [col for col in train_df.columns if "cap_count_" in col]
column_types["numerical_features"] += [col for col in train_df.columns if "mean_price_" in col]

training_features = column_types["boolean_features"] + \
  column_types["categorical_features"] + \
  column_types["numerical_features"] + \
  [col for col in train_df.columns if "tfidf_" in col] + \
  [col for col in train_df.columns if "sentiment_" in col]

len(training_features)

scale_features(train_df, training_features)

# manually turning some features off
training_features = [col for col in training_features if col not in ["latitude", "longitude"]]

train_df[training_features].head()

train_df["mean_price_street"]

"""# Feature Selection"""

# Simple Model
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error
from sklearn.model_selection import KFold

def eval_rf_model(df, features):
  
  kf = KFold(n_splits=5, random_state=2020, shuffle=True)
  mean_baseline_mae = 0
  mean_val_mae = 0

  for f_idx, (train_index, val_index) in enumerate(kf.split(df)):
    # print("", "Fold", f_idx)
    X_train = df.loc[train_index][features]
    X_val = df.loc[val_index][features]
    y_train = df.loc[train_index]["price"]
    y_val = df.loc[val_index]["price"]

    # TODO Try more models
    reg = RandomForestClassifier(n_estimators=100, max_depth=20, min_samples_leaf=3, random_state=2020, n_jobs=-1)
    reg.fit(X_train.values, y_train.values)

    # TODO Add paramters optimization

    # print("", "", "Train MAE", mean_absolute_error(y_train.values, reg.predict(X_train.values)))
    val_mae = mean_absolute_error(y_val.values, reg.predict(X_val.values))
    baseline_mae = mean_absolute_error(y_val.values, [y_train.values.mean()] * y_val.shape[0])
    # print("", "", "Val. MAE", val_mae)
    # print("", "", "Baseline, Guess By Mean Val. MAE", baseline_mae)

    mean_val_mae += val_mae/5
    mean_baseline_mae += baseline_mae/5

  print("", "Mean MAE over Kfold", mean_val_mae)
  print("", "Baseline (Guess By Mean) Mean MAE over Kfold", mean_baseline_mae)

  return pd.DataFrame(zip(training_features, reg.feature_importances_)).sort_values(1, ascending=False).reset_index(drop=True), reg

# with all training features
f_imp_df, rf_model = eval_rf_model(train_df, training_features)

"""## Feature Selection by Random Forest Feature Importance"""

pd.set_option('display.max_rows', 2000)
# Feature Importances, the higher the better (for this model)
f_imp_df.head(50)

# for i in range(10, 180, 10):
#   print(f"With Top {i} Features:")
#   eval_rf_model(train_df, f_imp_df[0].values[0: i])
#   print()

"""## Feature Selection by Correlation"""

corrs = []
for feature in training_features:
  corr = train_df["price"].corr(train_df[feature])
  corrs.append(abs(corr))

f_corr_df = pd.DataFrame(zip(training_features, corrs)).sort_values(1, ascending=False).reset_index(drop=True)
f_corr_df.head(50)

for i in range(10, 50, 10):
  print(f"With Top {i} Features:")
  eval_rf_model(train_df, f_corr_df[0].values[0: i])
  print()

"""## Feature Selection by SHapley Additive exPlanations"""

!pip install shap

_temp_df = train_df.sample(frac=.05)
_temp_df.shape

# # https://towardsdatascience.com/explain-your-model-with-the-shap-values-bc36aac4de3d
# # https://github.com/slundberg/shap

# import shap
# import numpy as np

# shap_values = shap.TreeExplainer(rf_model).shap_values(
#     _temp_df[training_features]
#   )

# shap_values = np.array(shap_values)
# mean_abs_shap_values = np.mean(np.mean(np.abs(shap_values), axis=1), axis=0)
# f_shap_df = pd.DataFrame(zip(training_features, mean_abs_shap_values)).sort_values(1, ascending=False).reset_index(drop=True)
# f_shap_df.head(50)

for i in range(10, 180, 10):
  print(f"With Top {i} Features:")
  eval_rf_model(train_df, f_shap_df[0].values[0: i])
  print()

"""## +/- Name Sentiment"""

train_df[["name", "trans_name", "sentiment_name"]].sort_values("sentiment_name").head(20)

train_df[["name", "trans_name", "sentiment_name"]].sort_values("sentiment_name", ascending=False).head(20)

"""## +/- Description Sentiment"""

train_df[["description", "trans_description", "sentiment_description"]].sort_values("sentiment_description").head(20)

train_df[["description", "trans_description", "sentiment_description"]].sort_values("sentiment_description", ascending=False).head(20)

"""# Regression"""

def eval_model(df, get_model, features):
  
  kf = KFold(n_splits=5, random_state=2020, shuffle=True)
  mean_baseline_mae = 0
  mean_val_mae = 0

  for f_idx, (train_index, val_index) in enumerate(kf.split(df)):
    # print("", "Fold", f_idx)
    X_train = df.loc[train_index][features]
    X_val = df.loc[val_index][features]
    y_train = df.loc[train_index]["price"]
    y_val = df.loc[val_index]["price"]

    reg = get_model()
    reg.fit(X_train.values, y_train.values)


    val_mae = mean_absolute_error(y_val.values, reg.predict(X_val.values))
    baseline_mae = mean_absolute_error(y_val.values, [y_train.values.mean()] * y_val.shape[0])
    print("", "Fold", f_idx, "Val. MAE", val_mae)
    # print("", "", "Baseline, Guess By Mean Val. MAE", baseline_mae)

    mean_val_mae += val_mae/5
    mean_baseline_mae += baseline_mae/5

  print("", "Mean MAE over Kfold", mean_val_mae)
  if hasattr(reg, "feature_importances_"):
    return pd.DataFrame(zip(training_features, reg.feature_importances_)).sort_values(1, ascending=False).reset_index(drop=True), reg
  else:
    return reg

selected_features = f_corr_df.head(30)[0].values
selected_features

from matplotlib import pyplot
from sklearn.model_selection import cross_val_score,KFold
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=0)

"""## Random Forest"""

_ = eval_model(train_df, 
           lambda : RandomForestRegressor(
               n_estimators=100, max_depth=20, min_samples_leaf=3, random_state=2020, n_jobs=-1
           ),
           selected_features,
           )

# Before Optimization
#  Fold 0 Val. MAE 179.96320107062334
#  Fold 1 Val. MAE 197.08395186626834
#  Fold 2 Val. MAE 182.0383690742713
#  Fold 3 Val. MAE 187.51407884597603
#  Fold 4 Val. MAE 190.69186510700874
#  Mean MAE over Kfold 187.45829319282953

"""RF after optimize"""

# Set the parameters by cross-validation
tuned_parameters = [{'n_estimators': [100, 150, 200], 
                     'max_depth': [15,20],
                     'min_samples_leaf':[3, 5],
                     'n_jobs':[-1],
                     }]

clf_rf = GridSearchCV(RandomForestRegressor( n_jobs=-1), tuned_parameters, 
                      cv=KFold(n_splits=5, random_state=2020, shuffle=True), verbose=1,
                   scoring='neg_mean_absolute_error')
clf_rf.fit(train_df[selected_features], train_df["price"])

print(clf_rf.best_params_)
print(-1 * clf_rf.best_score_ )

# Fitting 5 folds for each of 12 candidates, totalling 60 fits
# [Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.
# [Parallel(n_jobs=1)]: Done  60 out of  60 | elapsed:  3.7min finished
# {'max_depth': 20, 'min_samples_leaf': 3, 'n_estimators': 200, 'n_jobs': -1}
# 186.7619240412978

_, rf = eval_model(train_df, 
           lambda : RandomForestRegressor(
               n_estimators=200, max_depth=20, min_samples_leaf=3, random_state=2020, n_jobs=-1
           ),
           selected_features,
           )

# After optimization
#  Fold 0 Val. MAE 179.00724242942664
#  Fold 1 Val. MAE 196.48478377331875
#  Fold 2 Val. MAE 181.51231029430937
#  Fold 3 Val. MAE 187.303600439731
#  Fold 4 Val. MAE 189.91806880201676
#  Mean MAE over Kfold 186.84520114776052

"""## Analysis Predictions/Errors"""

from sklearn import tree
fig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (4,4), dpi=800)
tree.plot_tree(rf.estimators_[0],
               feature_names=selected_features, 
               class_names='price',
               filled =True);
fig.savefig('rf_individualtree.png')

"""## Gradient Tree Boosting

GBT before opt
"""

from sklearn.ensemble import GradientBoostingRegressor
_, gb = eval_model(train_df, 
           lambda : GradientBoostingRegressor(
              n_estimators=100, 
              max_depth=10, 
              learning_rate=0.1, 
              random_state=2020,
              loss='ls'
              ),
           selected_features,
           )

#  Fold 0 Val. MAE 182.8345787121608
#  Fold 1 Val. MAE 198.45354701964462
#  Fold 2 Val. MAE 182.48975867842591
#  Fold 3 Val. MAE 184.26930475659395
#  Fold 4 Val. MAE 190.55364676481807
#  Mean MAE over Kfold 187.7201671863287

# Set the parameters by cross-validation
tuned_parameters = [{'n_estimators': [100, 150, 200], 
                     'max_depth': [8, 10, 15],
                     'min_samples_leaf':[3, 5],
                     'learning_rate': [0.1], 'random_state': [2020], 
                     'loss': ['ls']
                     }]

clf_rf = GridSearchCV(GradientBoostingRegressor(), tuned_parameters, 
                      n_jobs=-1,
                      cv=KFold(n_splits=5, random_state=2020, shuffle=True),
                      verbose=1,
                   scoring='neg_mean_absolute_error')
clf_rf.fit(train_df[selected_features], train_df["price"])

print(clf_rf.best_params_)
print(-1 * clf_rf.best_score_ )

# Fitting 5 folds for each of 18 candidates, totalling 90 fits
# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
# [Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  2.1min
# [Parallel(n_jobs=-1)]: Done  90 out of  90 | elapsed:  5.7min finished
# {'learning_rate': 0.1, 'loss': 'ls', 'max_depth': 10, 'min_samples_leaf': 3, 'n_estimators': 100, 'random_state': 2020}
# 186.27377586791758

from sklearn.ensemble import GradientBoostingRegressor
_, gb = eval_model(train_df, 
           lambda : GradientBoostingRegressor(
              n_estimators=100, 
              max_depth=10, 
              learning_rate=0.1, 
              random_state=2020,
              min_samples_leaf=3,
              loss='ls'
              ),
           selected_features,
           )

#  Fold 0 Val. MAE 178.17960712626854
#  Fold 1 Val. MAE 197.02795726108516
#  Fold 2 Val. MAE 181.43453359082997
#  Fold 3 Val. MAE 185.26935241389478
#  Fold 4 Val. MAE 189.4574289475093
#  Mean MAE over Kfold 186.27377586791755

"""## Neural Network"""

from sklearn.neural_network import MLPRegressor
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)

nn = eval_model(train_df, 
           lambda : MLPRegressor(
              hidden_layer_sizes=(16),
              max_iter=2000,
              activation="relu",
              random_state=2020
            ),
           selected_features,
           )

#  Fold 0 Val. MAE 211.76026987183403
#  Fold 1 Val. MAE 216.8610533848067
#  Fold 2 Val. MAE 211.47808490133949
#  Fold 3 Val. MAE 219.3900815297341
#  Fold 4 Val. MAE 216.16027655864292
#  Mean MAE over Kfold 215.1299532492714

# Set the parameters by cross-validation
tuned_parameters = [{
          'hidden_layer_sizes': [(16), (32), (16, 16)],
          'activation': ['relu','tanh'],
          'max_iter': [2000],
          'solver': ['adam'],
          }]

clf_nn = GridSearchCV(MLPRegressor(), tuned_parameters, 
                      n_jobs=-1,
                      cv=KFold(n_splits=5, random_state=2020, shuffle=True),
                      verbose=1,
                   scoring='neg_mean_absolute_error')
clf_nn.fit(train_df[selected_features], train_df["price"])

print(clf_nn.best_params_)
print(-1 * clf_nn.best_score_ )

# Fitting 5 folds for each of 6 candidates, totalling 30 fits
# [Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.
# [Parallel(n_jobs=-1)]: Done  30 out of  30 | elapsed: 10.2min finished
# {'activation': 'relu', 'hidden_layer_sizes': 32, 'max_iter': 2000, 'solver': 'adam'}
# 212.32932994715765

from sklearn.neural_network import MLPRegressor
from sklearn.datasets import make_regression
from sklearn.model_selection import train_test_split
from warnings import simplefilter
from sklearn.exceptions import ConvergenceWarning
simplefilter("ignore", category=ConvergenceWarning)

nn = eval_model(train_df, 
           lambda : MLPRegressor(
              hidden_layer_sizes=(32),
              max_iter=2000,
              activation="relu",
              random_state=2020
            ),
           selected_features,
           )

# Fold 0 Val. MAE 210.23674666683485
#  Fold 1 Val. MAE 214.79418711974424
#  Fold 2 Val. MAE 207.5454305565766
#  Fold 3 Val. MAE 213.0805374932316
#  Fold 4 Val. MAE 215.80963342356955
#  Mean MAE over Kfold 212.29330705199138

"""## Ensemble the Models"""

from sklearn.ensemble import VotingRegressor

get_vote_reg = lambda: VotingRegressor(estimators=[
                                                   ('rf', RandomForestRegressor(
                                                            n_estimators=200,
                                                            max_depth=20,
                                                            min_samples_leaf=3,
                                                            random_state=2020,
                                                            n_jobs=-1,
                                                          )
                                                   ),
                                                   ('gb', GradientBoostingRegressor(
                                                            n_estimators=100, 
                                                            max_depth=10, 
                                                            learning_rate=0.1, 
                                                            random_state=2020,
                                                            min_samples_leaf=3,
                                                            loss='ls'
                                                            )
                                                   ),
                                                   ('nn', MLPRegressor(
                                                            hidden_layer_sizes=(32),
                                                            max_iter=2000,
                                                            activation="relu",
                                                            random_state=2020
                                                          )
                                                   ),
                                                   ], n_jobs=-1)

eval_model(train_df, get_vote_reg, selected_features)
#  Fold 0 Val. MAE 180.78657545609852
#  Fold 1 Val. MAE 194.2513043932482
#  Fold 2 Val. MAE 182.6202589050148
#  Fold 3 Val. MAE 187.26458041129501
#  Fold 4 Val. MAE 189.80048190560117
#  Mean MAE over Kfold 186.94464021425154

get_vote_reg = lambda: VotingRegressor(estimators=[
                                                   ('rf', RandomForestRegressor(
                                                            n_estimators=200,
                                                            max_depth=20,
                                                            min_samples_leaf=3,
                                                            random_state=2020,
                                                            n_jobs=-1,
                                                          )
                                                   ),
                                                   ('gb', GradientBoostingRegressor(
                                                            n_estimators=100, 
                                                            max_depth=10, 
                                                            learning_rate=0.1, 
                                                            random_state=2020,
                                                            min_samples_leaf=3,
                                                            loss='ls'
                                                            )
                                                   ),
                                                   ('nn', MLPRegressor(
                                                            hidden_layer_sizes=(32),
                                                            max_iter=2000,
                                                            activation="relu",
                                                            random_state=2020
                                                          )
                                                   ),
                                                   ], n_jobs=-1, weights=[.4, .4, .2])

eval_model(train_df, get_vote_reg, selected_features)

#  Fold 0 Val. MAE 177.66985053434527
#  Fold 1 Val. MAE 193.0907279175534
#  Fold 2 Val. MAE 180.0189458470157
#  Fold 3 Val. MAE 184.9692297364967
#  Fold 4 Val. MAE 187.69210712501777
#  Mean MAE over Kfold 184.68817223208575

get_vote_reg = lambda: VotingRegressor(estimators=[
                                                   ('rf', RandomForestRegressor(
                                                            n_estimators=200,
                                                            max_depth=20,
                                                            min_samples_leaf=3,
                                                            random_state=2020,
                                                            n_jobs=-1,
                                                          )
                                                   ),
                                                   ('gb', GradientBoostingRegressor(
                                                            n_estimators=100, 
                                                            max_depth=10, 
                                                            learning_rate=0.1, 
                                                            random_state=2020,
                                                            min_samples_leaf=3,
                                                            loss='ls'
                                                            )
                                                   ),
                                                   ('nn', MLPRegressor(
                                                            hidden_layer_sizes=(32),
                                                            max_iter=2000,
                                                            activation="relu",
                                                            random_state=2020
                                                          )
                                                   ),
                                                   ], n_jobs=-1, weights=[.45, .45, .1])

eval_model(train_df, get_vote_reg, selected_features)

#  Fold 0 Val. MAE 176.14900773399717
#  Fold 1 Val. MAE 192.85570818025656
#  Fold 2 Val. MAE 178.74204618173405
#  Fold 3 Val. MAE 183.94856952645932
#  Fold 4 Val. MAE 186.86781646172534
#  Mean MAE over Kfold 183.71262961683448